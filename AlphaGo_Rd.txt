在训练值网络时，使用完整比赛会导致“过拟合”。“过拟合”的意思是网络会“记住”它训练过的棋局，而不是真正的掌握下棋的规律。这种过拟合体现为训练集的MSE比测试集的MSE大不少。
所以在训练alphago的值网络时，使用了3千万张来自左右互搏的棋局截图（重点是这些截图不是连续的）。
得到的值网络在“位置评估”精度方面比单纯用快速策略的蒙特卡洛要好。而在和使用RL策略网络的蒙特卡洛比较时，虽然性能比RL要低一点点，但是使用了15000分之一的计算时间。
在AlphaGo中，SL策略网络比RL策略网络的效果要好，作者认为这是因为围棋毕竟是连续的，RL对于单步的预测要准，但是SL懂得“另辟蹊径”。但是SL推出的值函数比RL的效果要差。
在alphago的蒙特卡洛树搜索中，经过了expand后的一顿仿真后，叶节点的评估包含两部分：值网络对叶节点的评估（其实在仿真前就可以算出来，用GPU），加上使用快速仿真策略得到的结果。实验发现，即使单单使用值网络的评估结果来进行蒙特卡洛树搜索（仔细一想，不使用随机的仿真了，这都不能算是蒙特卡洛了。。。），这样的alphago也比其他的围棋程序要强，说明值网络可以替换蒙特卡洛仿真。然而将两者结合的效果最好，因为值网络是对速度慢的SL的近似，刚好和快速仿真互补。

首先是一个“最优值函数”，在完全信息游戏中，可以使用这个函数在搜索树中找出最优的走法。
但是围棋的搜索树太大了，为了减小搜索空间，有两种办法：一，针对深度，使用position evaluation，给搜索树中的节点计算一个近似的值函数的值；二，针对广度，使用policy来选取下一步的action，就好像蒙特卡洛的仿真阶段，根据一个策略（最简单的就是完全随机）仿真直到游戏分出胜负或者达到终止条件，在back阶段把仿真的收益反馈回去，这样的仿真次数多了的话，一平均就可以得到一个有效的position evaluation。

#蒙特卡洛树搜索有两个policy，UCB1算法算一个，仿真时的算一个，而超级多数量的仿真也可以改善UCB1算法的准确度。

使用MCTS的好处是随着仿真次数的增加，policy和value function都会渐进逼近最佳的。现在的顶尖围棋程序都是基于MCTS的，但是使用的policy和value function都较简单，导致程序只能达到业余水平。

深度卷积神经网络可以获得越来越抽象的，局部化的图像表示。把棋盘当做19x19的图像，通过卷积层得到这个棋盘表示。
AlphaGo就是利用这深度卷积神经网络来减少搜索树的深度和广度的：值网络用于position evaluation，policy network用于选择下一步（准确来说是在仿真时选择下一步，而不是selection时选择下一步，selection中的选择是根据UCB1的变种算法，利用仿真时积累的知识而做出来的）。

SL策略网络：交替权重，rectifier激活函数，softmax输出条件概率，随机梯度上升，（s,a）对为训练集，输入s，输出概率分布，KGS上的3千万张棋谱。
快速策略网络：快速简单。
RL策略网络：有一个reward函数，和SL都是用随机梯度上升更新参数
值网络：把RL策略网络看做一个棋手，值网络就是这个棋手的监督下学习这个棋手对于局势的判断。