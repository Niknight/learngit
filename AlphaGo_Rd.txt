在训练值网络时，使用完整比赛会导致“过拟合”。“过拟合”的意思是网络会“记住”它训练过的棋局，而不是真正的掌握下棋的规律。这种过拟合体现为训练集的MSE比测试集的MSE大不少。
所以在训练alphago的值网络时，使用了3千万张来自左右互搏的棋局截图（重点是这些截图不是连续的）。
得到的值网络在“位置评估”精度方面比单纯用仿真策略的蒙特卡洛要好。而在和使用RL策略网络的蒙特卡洛比较时，虽然性能比RL要低一点点，但是使用了15000分之一的计算时间。
在AlphaGo中，SL策略网络比RL策略网络的效果要好，作者认为这是因为围棋毕竟是连续的，RL对于单步的预测要准，但是SL懂得“另辟蹊径”。但是SL推出的值函数比RL的效果要差。
在alphago的蒙特卡洛树搜索中，经过了expand后的一顿仿真后，叶节点的评估包含两部分：值网络对叶节点的评估（其实在仿真前就可以算出来，用GPU），加上使用快速仿真策略得到的结果。实验发现，即使单单使用值网络的评估结果来进行蒙特卡洛树搜索（仔细一想，不使用随机的仿真了，这都不能算是蒙特卡洛了。。。），这样的alphago也比其他的围棋程序要强，说明值网络可以替换蒙特卡洛仿真。然而将两者结合的效果最好，因为值网络是对速度慢的SL的近似，刚好和快速仿真互补。